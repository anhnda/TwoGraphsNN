class Net5(torch.nn.Module):
    def __init__(self, numNode=10000, numAtomFeature=0):
        super(Net5, self).__init__()

        self.convD1 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.convD2 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)

        # self.my_reset_params(self.convD1.weight, config.EMBED_DIM)
        # self.my_reset_params(self.convD1.bias, config.EMBED_DIM)
        #
        # self.my_reset_params(self.convD2.weight, config.EMBED_DIM)
        # self.my_reset_params(self.convD2.bias, config.EMBED_DIM)

        self.convS1 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.convS2 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)

        # self.my_reset_params(self.convS1.weight, config.EMBED_DIM)
        # self.my_reset_params(self.convS1.bias, config.EMBED_DIM)
        #
        # self.my_reset_params(self.convS2.weight, config.EMBED_DIM)
        # self.my_reset_params(self.convS2.bias, config.EMBED_DIM)

        self.L1 = Linear(config.CHEM_FINGERPRINT_SIZE, config.EMBED_DIM * 2)
        self.actL1 = F.relu
        self.L2 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.actL2 = F.relu

        self.linear1 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.act1 = F.relu
        self.linear2 = Linear(config.EMBED_DIM, 1)
        self.act2 = F.relu

        self.nodesEmbedding = torch.nn.Embedding(num_embeddings=numNode + 1, embedding_dim=config.EMBED_DIM)
        self.nodesEmbedding.weight.data.uniform_(0.001, 0.3)

        # Molecule graph neural net

        self.mlinear1 = Linear(numAtomFeature, config.EMBED_DIM * 2)
        self.mact1 = F.relu
        self.mlinear2 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.mact2 = F.relu

        self.conv1 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool1 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.conv2 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool2 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.conv3 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool3 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.lin1 = torch.nn.Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.lin2 = torch.nn.Linear(config.EMBED_DIM, config.EMBED_DIM)

        self.bn1 = torch.nn.BatchNorm1d(128)
        self.bn2 = torch.nn.BatchNorm1d(64)
        self.act1 = torch.nn.ReLU()
        self.act2 = torch.nn.ReLU()

    def my_reset_params(self, tensor, size=10):
        bound = 1.0 / math.sqrt(size)
        if tensor is not None:
            tensor.data.uniform_(0.0, bound)

    def forward(self, x, drugEdges, seEdges, drugNodes, seNodes, drugGraphBatch, nDrug):

        # xDrug, edge_index, batch = drugGraphBatch.x, drugGraphBatch.edge_index, drugGraphBatch.batch
        #
        # # xDrug = self.mact1(self.mlinear1(xDrug))
        # # xDrug = self.mact2(self.mlinear2(xDrug))
        #
        # xDrug = self.nodesEmbedding(xDrug)
        # xDrug = xDrug.squeeze(1)
        #
        # xDrug = F.relu(self.conv1(xDrug, edge_index))
        #
        # v = self.pool1(xDrug, edge_index, None, batch)
        # xDrug, edge_index, _, batch, _, _ = v
        # x1 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)
        #
        # xDrug = F.relu(self.conv2(xDrug, edge_index))
        #
        # xDrug, edge_index, _, batch, _, _ = self.pool2(xDrug, edge_index, None, batch)
        # x2 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)
        #
        # xDrug = F.relu(self.conv3(xDrug, edge_index))
        #
        # xDrug, edge_index, _, batch, _, _ = self.pool3(xDrug, edge_index, None, batch)
        # x3 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)
        #
        # xDrug = x1 + x2 + x3
        #
        # xDrug = self.lin1(xDrug)
        # xDrug = self.act1(xDrug)
        # xDrug = self.lin2(xDrug)
        # xDrug = self.act2(xDrug)
        #
        # x = self.nodesEmbedding(x[nDrug:])
        # x = x.squeeze(1)
        #
        # x = torch.cat((xDrug, x), dim=0)

        x = self.nodesEmbedding(x)

        # Conv Drug:
        # x = self.convD1(x, drugEdges)
        # x = F.relu(x)
        # x = self.convD2(x, drugEdges)
        # x = F.relu(x)
        # # Conv SE:
        # x = self.convS1(x, seEdges)
        # x = F.relu(x)
        # x = self.convS2(x, seEdges)
        # x = F.relu(x)

        drugEmbedding = x[drugNodes]
        seEmbedding = x[seNodes]
        # re = torch.sigmoid(re)
        return drugEmbedding, seEmbedding, x

    def cal(self, drugE, seE):
        return torch.matmul(drugE, seE.t())

    def cal2(self, drugE, seE):
        nDrug, nDim = drugE.shape
        nSe, _ = seE.shape
        preRe = list()
        for i in range(nDrug):
            dE = drugE[i]
            dE = dE.squeeze()
            de = dE.expand((nSe, nDim))
            v = torch.cat((de, seE), dim=1)
            v = self.linear1(v)
            v = self.act1(v)
            v = self.linear2(v)
            # v = self.act2(v)
            v = v.squeeze()
            preRe.append(v)
        return torch.stack(preRe)

('Undirected graph: ', False)
MPNNX
<models.MPNNX5.MPNNX5 object at 0x7f0c9efc2fd0>
('Manual torch seed: ', 443181909)
Training raw path: /home/anhnd/DTI Project/Codes/MPNN/data/KFold/ATCInchikeySideEffectByDrug.txt_train_0
('Number of substructures, proteins, pathways, drugs, se: ', 2936, 1448, 330, 969, 598)
((872, 330), (97, 330), (872, 598), (97, 598))
((872, 598), (872, 598), 1177701.9, 88676.0)
('Error: ', tensor(2368344.2500, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5039200352637093, 0.17197922255640505)
('Test: AUC, AUPR: ', 0.5379326397911255, 0.19927647134785123)
((872, 598), (872, 598), 217287.33, 88676.0)
('Error: ', tensor(108073.0312, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5561293399052856, 0.20160150665583762)
('Test: AUC, AUPR: ', 0.5690282594655085, 0.22952385730740452)
((872, 598), (872, 598), 31681.691, 88676.0)
('Error: ', tensor(77905.5156, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.6199329723990219, 0.26930745181595206)
('Test: AUC, AUPR: ', 0.6643508903690631, 0.3797468858883299)
((872, 598), (872, 598), 7063.0723, 88676.0)
('Error: ', tensor(84325.3281, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.6161470220241668, 0.2819204872937551)
('Test: AUC, AUPR: ', 0.702960068555533, 0.4297325927509573)
((872, 598), (872, 598), 4718.335, 88676.0)
('Error: ', tensor(84019.2188, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.6314994916559737, 0.3217038482243991)
('Test: AUC, AUPR: ', 0.7194864408009432, 0.44529016314128284)
((872, 598), (872, 598), 7645.4688, 88676.0)
('Error: ', tensor(80262.6875, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.6778667532562059, 0.40303100025749405)
('Test: AUC, AUPR: ', 0.7291704323595103, 0.4529568733971575)
((872, 598), (872, 598), 15761.619, 88676.0)
('Error: ', tensor(73382.7891, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.7384578512291061, 0.5007048318927313)
('Test: AUC, AUPR: ', 0.7358850596352858, 0.4576513486161282)
((872, 598), (872, 598), 31479.701, 88676.0)
('Error: ', tensor(64107.9141, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.7920500462455842, 0.5755318800452405)
('Test: AUC, AUPR: ', 0.740534378319444, 0.4605056959805071)
((872, 598), (872, 598), 55269.98, 88676.0)
('Error: ', tensor(55733.3555, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8264119210499095, 0.6145685823277917)
('Test: AUC, AUPR: ', 0.7434674002802625, 0.4621331199438255)
((872, 598), (872, 598), 80431.64, 88676.0)
('Error: ', tensor(51924.4922, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8438388810430149, 0.6308981476862534)
('Test: AUC, AUPR: ', 0.7450605991670529, 0.4632236221285793)
((872, 598), (872, 598), 95581.1, 88676.0)
('Error: ', tensor(51029.6641, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8522851232931345, 0.6388465937776177)
('Test: AUC, AUPR: ', 0.7457458318402999, 0.4636584912324181)
((872, 598), (872, 598), 98200.914, 88676.0)
('Error: ', tensor(50305.1367, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8578416539024731, 0.6458620295476992)
('Test: AUC, AUPR: ', 0.7459117461414896, 0.4631781996636786)
((872, 598), (872, 598), 95644.27, 88676.0)
('Error: ', tensor(49648.0234, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8633390444483733, 0.6545771484585264)
('Test: AUC, AUPR: ', 0.7459031189583368, 0.46304182834252083)
((872, 598), (872, 598), 93753.2, 88676.0)
('Error: ', tensor(49005.3359, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8692255404600229, 0.664532384161736)
('Test: AUC, AUPR: ', 0.7458993025689004, 0.46339159313269707)
((872, 598), (872, 598), 93416.83, 88676.0)
('Error: ', tensor(48279.2969, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8751182212377433, 0.6746264678470424)
('Test: AUC, AUPR: ', 0.7458998052788182, 0.46365824366141595)
((872, 598), (872, 598), 93470.72, 88676.0)
('Error: ', tensor(47469.8008, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8809464679231798, 0.6850144595052614)
('Test: AUC, AUPR: ', 0.7458497726236031, 0.46377440094384403)
Train: 0.8863 0.6952
Test: 0.7458 0.4638
[0.516430414212755, 0.520633500736749, 0.5285329749029735, 0.5424950743112491, 0.5454663369434295, 0.5583876667724403, 0.5671340198733262, 0.5752093180168791, 0.5795979035441187, 0.5840965710177161, 0.5878060437891786, 0.5925012833724735, 0.6036921346556615, 0.6109020227707791, 0.6133987431513628, 0.6166986857753963, 0.6182936721828569, 0.6215160613336657, 0.6240099086452147, 0.6306573812846423, 0.6364812066974979, 0.6410660791641634, 0.6459227131188876, 0.6498729930127629, 0.6539237949668765, 0.6544932869012922, 0.6575707779316562, 0.6608003726983958, 0.6651834360400162, 0.6666289457425871, 0.6723471230327517, 0.6750947060693662, 0.6761455826210342, 0.6778507618858518, 0.6790363135836461, 0.6856233645127293, 0.6883112353829838, 0.6925153849447145, 0.6951542618143098, 0.6993716622636097, 0.701148727302265, 0.7023894859280353, 0.706318359709085, 0.706318359709085, 0.7088146483670984, 0.7098995472345292, 0.7121664244525383, 0.7133408683314165, 0.7174967313219474, 0.7210321328486218, 0.7210321328486218, 0.7223259755071856, 0.7223259755071856, 0.7249065975405711, 0.7260564624450057, 0.7260564624450057, 0.7282648742121152, 0.7282648742121152, 0.7282648742121152, 0.7282648742121152, 0.7282648742121152, 0.7282648742121152, 0.7282648742121152, 0.7314206014762863, 0.7314206014762863, 0.7330169661227363, 0.734551766604349, 0.734551766604349, 0.734551766604349, 0.734551766604349, 0.734551766604349, 0.734551766604349, 0.736390263274417, 0.736390263274417, 0.736390263274417, 0.7417666573735023, 0.7417666573735023, 0.7417666573735023, 0.743727243239054, 0.743727243239054, 0.743727243239054, 0.743727243239054, 0.743727243239054, 0.7458497726236031, 0.7458497726236031, 0.7458497726236031, 0.7458497726236031, 0.7458497726236031, 0.7458497726236031, 0.7458497726236031, 0.7458497726236031, 0.7458497726236031, 0.7458497726236031, 0.7458497726236031, 0.7458497726236031]
[0.06269248692750272, 0.07111588081448857, 0.07824844736753991, 0.08552125486717943, 0.08984465693959655, 0.09922428772717584, 0.1076478428313157, 0.11363653001935958, 0.11794430912286503, 0.12259559197706402, 0.1262411797861964, 0.1317064424669156, 0.1448956858326616, 0.1532120573151817, 0.15684874072798424, 0.16095389005937305, 0.16341799849877145, 0.1665836727628126, 0.1708346434788908, 0.18185523523909558, 0.1916973982176543, 0.19805422874172923, 0.2047110355072415, 0.21062192899790283, 0.21855184739204908, 0.21940927887674488, 0.22436663115064476, 0.22968035374644877, 0.23855350189052255, 0.24154812517656826, 0.2532441886628214, 0.2589014091430574, 0.2612148524256957, 0.26396653518420166, 0.26736774350378445, 0.2825540740724548, 0.28728452032720275, 0.2967555170514838, 0.30270218452916586, 0.3125696965509612, 0.31649790398629585, 0.31949089523743607, 0.3284864431713109, 0.3284864431713109, 0.3356867658178852, 0.33843476908703896, 0.34527642547717086, 0.34854339772836046, 0.35869501530618253, 0.36930439563761797, 0.36930439563761797, 0.37321734378489985, 0.37321734378489985, 0.3815307097894433, 0.3849992125617971, 0.3849992125617971, 0.3908209032757554, 0.3908209032757554, 0.3908209032757554, 0.3908209032757554, 0.3908209032757554, 0.3908209032757554, 0.3908209032757554, 0.403671366130415, 0.403671366130415, 0.40935081132502693, 0.41561271167975117, 0.41561271167975117, 0.41561271167975117, 0.41561271167975117, 0.41561271167975117, 0.41561271167975117, 0.42328906036013114, 0.42328906036013114, 0.42328906036013114, 0.4455192817096066, 0.4455192817096066, 0.4455192817096066, 0.45426435103817037, 0.45426435103817037, 0.45426435103817037, 0.45426435103817037, 0.45426435103817037, 0.46377440094384403, 0.46377440094384403, 0.46377440094384403, 0.46377440094384403, 0.46377440094384403, 0.46377440094384403, 0.46377440094384403, 0.46377440094384403, 0.46377440094384403, 0.46377440094384403, 0.46377440094384403, 0.46377440094384403]
