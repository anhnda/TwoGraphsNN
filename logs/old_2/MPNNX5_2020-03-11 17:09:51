class Net5(torch.nn.Module):
    def __init__(self, numNode=10000, numAtomFeature=0):
        super(Net5, self).__init__()

        self.convD1 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.convD2 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)

        # self.my_reset_params(self.convD1.weight, config.EMBED_DIM)
        # self.my_reset_params(self.convD1.bias, config.EMBED_DIM)
        #
        # self.my_reset_params(self.convD2.weight, config.EMBED_DIM)
        # self.my_reset_params(self.convD2.bias, config.EMBED_DIM)

        self.convS1 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.convS2 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)

        # self.my_reset_params(self.convS1.weight, config.EMBED_DIM)
        # self.my_reset_params(self.convS1.bias, config.EMBED_DIM)
        #
        # self.my_reset_params(self.convS2.weight, config.EMBED_DIM)
        # self.my_reset_params(self.convS2.bias, config.EMBED_DIM)

        self.L1 = Linear(config.CHEM_FINGERPRINT_SIZE, config.EMBED_DIM * 2)
        self.actL1 = F.relu
        self.L2 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.actL2 = F.relu

        self.linear1 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.act1 = F.relu
        self.linear2 = Linear(config.EMBED_DIM, 1)
        self.act2 = F.relu

        self.nodesEmbedding = torch.nn.Embedding(num_embeddings=numNode + 1, embedding_dim=config.EMBED_DIM)
        # self.nodesEmbedding.weight.data.uniform_(0.001, 0.3)

        # Molecule graph neural net

        self.mlinear1 = Linear(numAtomFeature, config.EMBED_DIM * 2)
        self.mact1 = F.relu
        self.mlinear2 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.mact2 = F.relu

        self.conv1 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool1 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.conv2 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool2 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.conv3 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool3 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.lin1 = torch.nn.Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.lin2 = torch.nn.Linear(config.EMBED_DIM, config.EMBED_DIM)

        self.bn1 = torch.nn.BatchNorm1d(128)
        self.bn2 = torch.nn.BatchNorm1d(64)
        self.act1 = torch.nn.ReLU()
        self.act2 = torch.nn.ReLU()

    def my_reset_params(self, tensor, size=10):
        bound = 1.0 / math.sqrt(size)
        if tensor is not None:
            tensor.data.uniform_(0.0, bound)

    def forward(self, x, drugEdges, seEdges, drugNodes, seNodes, drugGraphBatch, nDrug):

        # xDrug, edge_index, batch = drugGraphBatch.x, drugGraphBatch.edge_index, drugGraphBatch.batch
        #
        # # xDrug = self.mact1(self.mlinear1(xDrug))
        # # xDrug = self.mact2(self.mlinear2(xDrug))
        #
        # xDrug = self.nodesEmbedding(xDrug)
        # xDrug = xDrug.squeeze(1)
        #
        # xDrug = F.relu(self.conv1(xDrug, edge_index))
        #
        # v = self.pool1(xDrug, edge_index, None, batch)
        # xDrug, edge_index, _, batch, _, _ = v
        # x1 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)
        #
        # xDrug = F.relu(self.conv2(xDrug, edge_index))
        #
        # xDrug, edge_index, _, batch, _, _ = self.pool2(xDrug, edge_index, None, batch)
        # x2 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)
        #
        # xDrug = F.relu(self.conv3(xDrug, edge_index))
        #
        # xDrug, edge_index, _, batch, _, _ = self.pool3(xDrug, edge_index, None, batch)
        # x3 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)
        #
        # xDrug = x1 + x2 + x3
        #
        # xDrug = self.lin1(xDrug)
        # xDrug = self.act1(xDrug)
        # xDrug = self.lin2(xDrug)
        # xDrug = self.act2(xDrug)
        #
        # x = self.nodesEmbedding(x[nDrug:])
        # x = x.squeeze(1)
        #
        # x = torch.cat((xDrug, x), dim=0)

        x = self.nodesEmbedding(x)

        # Conv Drug:
        # x = self.convD1(x, drugEdges)
        # x = F.relu(x)
        # x = self.convD2(x, drugEdges)
        # x = F.relu(x)
        # # Conv SE:
        # x = self.convS1(x, seEdges)
        # x = F.relu(x)
        # x = self.convS2(x, seEdges)
        # x = F.relu(x)

        drugEmbedding = x[drugNodes]
        seEmbedding = x[seNodes]
        # re = torch.sigmoid(re)
        return drugEmbedding, seEmbedding, x

    def cal(self, drugE, seE):
        return torch.matmul(drugE, seE.t())

    def cal2(self, drugE, seE):
        nDrug, nDim = drugE.shape
        nSe, _ = seE.shape
        preRe = list()
        for i in range(nDrug):
            dE = drugE[i]
            dE = dE.squeeze()
            de = dE.expand((nSe, nDim))
            v = torch.cat((de, seE), dim=1)
            v = self.linear1(v)
            v = self.act1(v)
            v = self.linear2(v)
            # v = self.act2(v)
            v = v.squeeze()
            preRe.append(v)
        return torch.stack(preRe)

('Undirected graph: ', False)
MPNNX
<models.MPNNX5.MPNNX5 object at 0x7f3ffd14a2d0>
('Manual torch seed: ', 443181909)
Training raw path: /home/anhnd/DTI Project/Codes/MPNN/data/KFold/ATCInchikeySideEffectByDrug.txt_train_0
('Number of substructures, proteins, pathways, drugs, se: ', 2936, 1448, 330, 969, 598)
((872, 330), (97, 330), (872, 598), (97, 598))
((872, 598), (872, 598), -4310.666, 88676.0)
('Error: ', tensor(51953784., grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5005089897482482, 0.1703299870229478)
('Test: AUC, AUPR: ', 0.5010783838742964, 0.18209182088984638)
((872, 598), (872, 598), -1295.0896, 88676.0)
('Error: ', tensor(36869864., grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5011361807484144, 0.17061020639109784)
('Test: AUC, AUPR: ', 0.5010480139864875, 0.1820093348901281)
((872, 598), (872, 598), 1212.3125, 88676.0)
('Error: ', tensor(26298182., grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5019284191626399, 0.1709519902788155)
('Test: AUC, AUPR: ', 0.5010214474697307, 0.18192214674713372)
((872, 598), (872, 598), 3379.8572, 88676.0)
('Error: ', tensor(19035312., grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5029348487542888, 0.17139147543752029)
('Test: AUC, AUPR: ', 0.5010111068669175, 0.1818530854444794)
((872, 598), (872, 598), 5243.259, 88676.0)
('Error: ', tensor(14029120., grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5041411576660525, 0.17192339984064808)
('Test: AUC, AUPR: ', 0.5010552221657485, 0.18185551726888374)
((872, 598), (872, 598), 6817.8315, 88676.0)
('Error: ', tensor(10513676., grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5055312679919982, 0.17254389766587996)
('Test: AUC, AUPR: ', 0.50110401507021, 0.18189850524683057)
((872, 598), (872, 598), 8152.51, 88676.0)
('Error: ', tensor(7981383., grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5071204215817388, 0.17326052937874747)
('Test: AUC, AUPR: ', 0.5012171368187069, 0.18200932048192284)
((872, 598), (872, 598), 9323.575, 88676.0)
('Error: ', tensor(6110547., grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5089314673277534, 0.1740806766042422)
('Test: AUC, AUPR: ', 0.5013451085380655, 0.18214363321911378)
((872, 598), (872, 598), 10383.092, 88676.0)
('Error: ', tensor(4699666., grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5109952800990327, 0.17502093460081225)
('Test: AUC, AUPR: ', 0.5014936653272892, 0.1823150569218026)
((872, 598), (872, 598), 11369.647, 88676.0)
('Error: ', tensor(3620762.2500, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.513359832299362, 0.1761140553131099)
('Test: AUC, AUPR: ', 0.5016559154539977, 0.1824831139973294)
((872, 598), (872, 598), 12306.5625, 88676.0)
('Error: ', tensor(2789678.2500, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5160879764960274, 0.1773955514718301)
('Test: AUC, AUPR: ', 0.5018348391267877, 0.1826596035894256)
((872, 598), (872, 598), 13219.74, 88676.0)
('Error: ', tensor(2148294., grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5192596939815041, 0.17892169046709172)
('Test: AUC, AUPR: ', 0.5020128394957047, 0.18282127524689762)
((872, 598), (872, 598), 14136.83, 88676.0)
('Error: ', tensor(1654271.1250, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5229603214025889, 0.1807524017674523)
('Test: AUC, AUPR: ', 0.5021810700678906, 0.18295115913815538)
((872, 598), (872, 598), 15083.639, 88676.0)
('Error: ', tensor(1275317.3750, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5272875819535312, 0.18295330476449806)
('Test: AUC, AUPR: ', 0.5023434834251103, 0.18308651369111129)
((872, 598), (872, 598), 16086.862, 88676.0)
('Error: ', tensor(986050.5000, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5323484718997451, 0.18561547348753157)
('Test: AUC, AUPR: ', 0.5024984082070988, 0.18318656461209268)
((872, 598), (872, 598), 17175.627, 88676.0)
('Error: ', tensor(766255.1250, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5382609171864507, 0.18884405645311667)
('Test: AUC, AUPR: ', 0.5026447568780575, 0.1832683975367345)
Train: 0.5444 0.1923
Test: 0.5026 0.1833
[0.5086878624455626, 0.5075670249569102, 0.5188235227687663, 0.5151202065057274, 0.517854144229382, 0.5151470696201097, 0.5086527812269841, 0.503937624833498, 0.5014843281213008, 0.5021329297737357, 0.5010505234124492, 0.5014658888545971, 0.5006675425569179, 0.5000920918809606, 0.4993305441724336, 0.500041850053758, 0.5013554951806299, 0.5006719155576499, 0.5011967301486027, 0.5015510690406936, 0.5005793595714824, 0.5010869892887001, 0.5013349055785328, 0.501160087956523, 0.5016136885761409, 0.5015170594739172, 0.50234824582608, 0.5033165997060092, 0.5034865108910405, 0.5032001849359066, 0.5027598587779277, 0.5021176168904247, 0.5018985772781759, 0.501101152374938, 0.5014070897478409, 0.5014537530692087, 0.5022548497776251, 0.502071184033771, 0.5012347686607264, 0.5012032028895136, 0.501749722596966, 0.5017902909113283, 0.5019924957595127, 0.5019924957595127, 0.5021497158688315, 0.5021026932538966, 0.5019115875922266, 0.5019387048687616, 0.501608051210778, 0.5013329807529814, 0.5013329807529814, 0.5010975739932702, 0.5010975739932702, 0.5014715049246871, 0.5014375266821339, 0.5014375266821339, 0.5007463123082164, 0.5007463123082164, 0.5007463123082164, 0.5007463123082164, 0.5007463123082164, 0.5007463123082164, 0.5007463123082164, 0.5018340805898984, 0.5018340805898984, 0.5017504649454788, 0.5022370986960677, 0.5022370986960677, 0.5022370986960677, 0.5022370986960677, 0.5022370986960677, 0.5022370986960677, 0.5021063392478994, 0.5021063392478994, 0.5021063392478994, 0.5025116150832736, 0.5025116150832736, 0.5025116150832736, 0.5032417107665462, 0.5032417107665462, 0.5032417107665462, 0.5032417107665462, 0.5032417107665462, 0.5026447568780575, 0.5026447568780575, 0.5026447568780575, 0.5026447568780575, 0.5026447568780575, 0.5026447568780575, 0.5026447568780575, 0.5026447568780575, 0.5026447568780575, 0.5026447568780575, 0.5026447568780575, 0.5026447568780575]
[0.058560861205249205, 0.06472112908875431, 0.07210127999725065, 0.07574493954094631, 0.07907850857386828, 0.08374386969618723, 0.08792925338773404, 0.09141650534663696, 0.09323405230905148, 0.09583373231573948, 0.09704080101155016, 0.09930678548321338, 0.10356078872811117, 0.10663173899140918, 0.10762267228604651, 0.10951073065484655, 0.11069042662829486, 0.11231684151675214, 0.11350586593472804, 0.11650716899125681, 0.11867774750172053, 0.12099235358919577, 0.12333834391837589, 0.12556416897131614, 0.12782149899317063, 0.12807864273053954, 0.12996250909723264, 0.13205426552576363, 0.13422132920533245, 0.1349279628701556, 0.1379850801629038, 0.13920777596658676, 0.13967605560443974, 0.14047260999134747, 0.14114510674511022, 0.1448791587487633, 0.14671362345329753, 0.14922302622926328, 0.15058763058176713, 0.1529218105810209, 0.1543572106571088, 0.15508641059207454, 0.15765739175766094, 0.15765739175766094, 0.15913246610209708, 0.15974525593032013, 0.16107689484670867, 0.16195316831832213, 0.16458006228132452, 0.16709162649351006, 0.16709162649351006, 0.1677565001846065, 0.1677565001846065, 0.16952139070136335, 0.1704533544703294, 0.1704533544703294, 0.17162641106167653, 0.17162641106167653, 0.17162641106167653, 0.17162641106167653, 0.17162641106167653, 0.17162641106167653, 0.17162641106167653, 0.1739612231531196, 0.1739612231531196, 0.1749973560291782, 0.17621584089466674, 0.17621584089466674, 0.17621584089466674, 0.17621584089466674, 0.17621584089466674, 0.17621584089466674, 0.17730976577137908, 0.17730976577137908, 0.17730976577137908, 0.180738787778742, 0.180738787778742, 0.180738787778742, 0.1822314297513751, 0.1822314297513751, 0.1822314297513751, 0.1822314297513751, 0.1822314297513751, 0.1832683975367345, 0.1832683975367345, 0.1832683975367345, 0.1832683975367345, 0.1832683975367345, 0.1832683975367345, 0.1832683975367345, 0.1832683975367345, 0.1832683975367345, 0.1832683975367345, 0.1832683975367345, 0.1832683975367345]
