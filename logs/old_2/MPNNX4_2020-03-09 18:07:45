    def __init__(self, numNode=10000, numAtomFeature=0):
        super(Net4, self).__init__()

        self.convD1 = GATConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.convD2 = GATConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)

        self.convS1 = GATConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.convS2 = GATConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)

        self.L1 = Linear(config.CHEM_FINGERPRINT_SIZE, config.EMBED_DIM * 2)
        self.actL1 = F.relu
        self.L2 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.actL2 = F.relu

        self.linear1 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.act1 = F.relu
        self.linear2 = Linear(config.EMBED_DIM, 1)
        self.act2 = F.relu

        self.nodesEmbedding = torch.nn.Embedding(num_embeddings=numNode + 1, embedding_dim=config.EMBED_DIM)
        self.nodesEmbedding.weight.data.uniform_(0.001, 0.3)



        # Molecule graph neural net

        self.mlinear1 = Linear(numAtomFeature, config.EMBED_DIM * 2)
        self.mact1 = F.relu
        self.mlinear2 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.mact2 = F.relu

        self.conv1 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool1 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.conv2 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool2 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.conv3 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool3 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.lin1 = torch.nn.Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.lin2 = torch.nn.Linear(config.EMBED_DIM, config.EMBED_DIM)


        self.bn1 = torch.nn.BatchNorm1d(128)
        self.bn2 = torch.nn.BatchNorm1d(64)
        self.act1 = torch.nn.ReLU()
        self.act2 = torch.nn.ReLU()

    def forward(self, x, drugEdges, seEdges, drugNodes, seNodes, drugGraphBatch, nDrug):

        x = self.nodesEmbedding(x[nDrug:])
        x = x.squeeze(1)


        xDrug, edge_index, batch = drugGraphBatch.x, drugGraphBatch.edge_index, drugGraphBatch.batch

        xDrug = self.mact1(self.mlinear1(xDrug))
        xDrug = self.mact2(self.mlinear2(xDrug))

        xDrug = F.relu(self.conv1(xDrug, edge_index))

        v  = self.pool1(xDrug, edge_index, None, batch)
        xDrug, edge_index, _, batch, _, _ = v
        x1 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)

        xDrug = F.relu(self.conv2(xDrug, edge_index))

        xDrug, edge_index, _, batch, _, _ = self.pool2(xDrug, edge_index, None, batch)
        x2 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)

        xDrug = F.relu(self.conv3(xDrug, edge_index))

        xDrug, edge_index, _, batch, _, _ = self.pool3(xDrug, edge_index, None, batch)
        x3 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)

        xDrug = x1 + x2 + x3

        xDrug = self.lin1(xDrug)
        xDrug = self.act1(xDrug)
        xDrug = self.lin2(xDrug)
        xDrug = self.act2(xDrug)



        x = torch.cat((xDrug, x), dim=0)

        # Conv Drug:
        x = self.convD1(x, drugEdges)
        x = F.relu(x)
        x = self.convD2(x, drugEdges)
        x = F.relu(x)
        # Conv SE:
        # x = self.convS1(x, seEdges)
        # x = F.relu(x)
        # x = self.convS2(x, seEdges)
        # x = F.relu(x)

        drugEmbedding = x[drugNodes]
        seEmbedding = x[seNodes]
        # re = torch.sigmoid(re)
        return drugEmbedding, seEmbedding, x

('Undirected graph: ', True)
MPNNX
<models.MPNNX4.MPNNX4 object at 0x7f1b085c6210>
Training raw path: /home/anhnd/DTI Project/Codes/MPNN/data/KFold/ATCInchikeySideEffectByDrug.txt_train_0
('Number of substructures, proteins, pathways, drugs, se: ', 888, 1448, 330, 969, 598)
((872, 598), (872, 598), 41795.375, 88676.0)
('Error: ', tensor(78382.6953, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.471373114684986, 0.15585470717545893)
('Test: AUC, AUPR: ', 0.473252382496018, 0.17107043950780118)
((872, 598), (872, 598), 154700.72, 88676.0)
('Error: ', tensor(78803.3828, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.64581635954389, 0.328415038425231)
('Test: AUC, AUPR: ', 0.621336383056097, 0.3131041709043919)
((872, 598), (872, 598), 126203.61, 88676.0)
('Error: ', tensor(63356.0625, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.7727759334813032, 0.48394355575811265)
('Test: AUC, AUPR: ', 0.7363097183306516, 0.44279435716032406)
((872, 598), (872, 598), 92712.2, 88676.0)
('Error: ', tensor(58128.7461, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.7947718851488841, 0.5232827851613332)
('Test: AUC, AUPR: ', 0.7480716132669665, 0.45080832853856245)
((872, 598), (872, 598), 83256.57, 88676.0)
('Error: ', tensor(56087.9570, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8091280968666872, 0.5563144304310487)
('Test: AUC, AUPR: ', 0.748580938526931, 0.42607816510211555)
((872, 598), (872, 598), 97917.766, 88676.0)
('Error: ', tensor(54835.4414, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8204457900972705, 0.5750888514125404)
('Test: AUC, AUPR: ', 0.7483155257157365, 0.41202497580393455)
((872, 598), (872, 598), 91455.28, 88676.0)
('Error: ', tensor(53889.5117, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8289642601037664, 0.5885591393190359)
('Test: AUC, AUPR: ', 0.7504380280753635, 0.4253338603543667)
((872, 598), (872, 598), 93120.03, 88676.0)
('Error: ', tensor(53169.5156, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8338462092041439, 0.5975955012195876)
('Test: AUC, AUPR: ', 0.7520320971910754, 0.4334886382047178)
((872, 598), (872, 598), 94522.3, 88676.0)
('Error: ', tensor(52420.0586, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8383600053969337, 0.6075655583951782)
('Test: AUC, AUPR: ', 0.747551695047816, 0.42416062890350426)
((872, 598), (872, 598), 91510.76, 88676.0)
('Error: ', tensor(51475.1055, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8428495765415844, 0.6166022926519432)
('Test: AUC, AUPR: ', 0.7419868164263946, 0.4099874935298304)
((872, 598), (872, 598), 90361.164, 88676.0)
('Error: ', tensor(50417.2695, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8482204069791098, 0.6283748702992323)
('Test: AUC, AUPR: ', 0.7419751619681786, 0.40129448510726584)
((872, 598), (872, 598), 92547.96, 88676.0)
('Error: ', tensor(49561.3711, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8533963937558082, 0.6394399426042756)
('Test: AUC, AUPR: ', 0.7415697174066668, 0.3934743298556346)
((872, 598), (872, 598), 86090.01, 88676.0)
('Error: ', tensor(48892.1641, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8571550249406319, 0.6491036050122873)
('Test: AUC, AUPR: ', 0.7440290223929009, 0.3945552990053978)
((872, 598), (872, 598), 87401.34, 88676.0)
('Error: ', tensor(48154.9414, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8609180566289619, 0.6581680310364465)
('Test: AUC, AUPR: ', 0.7451077597664023, 0.3952002553910881)
((872, 598), (872, 598), 91917.88, 88676.0)
('Error: ', tensor(47681.3906, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8638423888133193, 0.6641214247228701)
('Test: AUC, AUPR: ', 0.7445549510999542, 0.3925017845110455)
((872, 598), (872, 598), 89135.21, 88676.0)
('Error: ', tensor(47288.3828, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8664628114962328, 0.6687458869909227)
('Test: AUC, AUPR: ', 0.7439196639588888, 0.39201649062068805)
((872, 598), (872, 598), 89235.914, 88676.0)
('Error: ', tensor(46999.9531, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8686348517848382, 0.6724065059088244)
('Test: AUC, AUPR: ', 0.7428048075787762, 0.3897681227544715)
((872, 598), (872, 598), 90117.99, 88676.0)
('Error: ', tensor(46727.0703, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8706048227029453, 0.6758935430236022)
('Test: AUC, AUPR: ', 0.7417375233792407, 0.389006132705747)
((872, 598), (872, 598), 87836.58, 88676.0)
('Error: ', tensor(46481.9062, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8725025723529036, 0.6794596592177895)
('Test: AUC, AUPR: ', 0.7410177839762111, 0.3922022117539085)
Train: 0.8725 0.6795
Test: 0.7410 0.3922
[0.5942408857398591, 0.5912829770861959, 0.6048096830780466, 0.6034383174188584, 0.6022829937125276, 0.6086518401944161, 0.6132556142611658, 0.6169207354901938, 0.6203447970903797, 0.6239434782942541, 0.624475909220786, 0.626185403990558, 0.632767760095429, 0.6370813842958142, 0.6385858721658454, 0.6406479227844547, 0.6414301937210133, 0.6431554304758071, 0.6438839487473519, 0.6484359220090679, 0.6520542769177846, 0.6553953012777958, 0.6583392799254643, 0.6616801134587744, 0.6645525460626612, 0.6650844260297899, 0.6673062173827902, 0.6698775607929253, 0.6730348765234899, 0.6741791259732661, 0.6781814349456079, 0.680075975110763, 0.6808309631737073, 0.6821985891532331, 0.6831701759780846, 0.6882680311709235, 0.6906307655533876, 0.6940253466032343, 0.6962240606824092, 0.6996635919442274, 0.701206264366893, 0.7022331942687083, 0.7055048753173404, 0.7055048753173404, 0.7076385340352722, 0.708493397808384, 0.7105137330766528, 0.7115093516983876, 0.7152456922870025, 0.7183417757360715, 0.7183417757360715, 0.719389985313333, 0.719389985313333, 0.7216702573323791, 0.7227445714935229, 0.7227445714935229, 0.724752499020247, 0.724752499020247, 0.724752499020247, 0.724752499020247, 0.724752499020247, 0.724752499020247, 0.724752499020247, 0.7275723734990689, 0.7275723734990689, 0.729030425943495, 0.7304388573123356, 0.7304388573123356, 0.7304388573123356, 0.7304388573123356, 0.7304388573123356, 0.7304388573123356, 0.7321731650780914, 0.7321731650780914, 0.7321731650780914, 0.7371936862770747, 0.7371936862770747, 0.7371936862770747, 0.7390189674147182, 0.7390189674147182, 0.7390189674147182, 0.7390189674147182, 0.7390189674147182, 0.7410177839762111, 0.7410177839762111, 0.7410177839762111, 0.7410177839762111, 0.7410177839762111, 0.7410177839762111, 0.7410177839762111, 0.7410177839762111, 0.7410177839762111, 0.7410177839762111, 0.7410177839762111, 0.7410177839762111]
[0.08867152272585489, 0.09948784607518674, 0.10998838015909668, 0.11456629094100482, 0.11974407515473963, 0.1278765060805767, 0.13441223700660188, 0.14105952153263027, 0.14668698941412953, 0.1522909517499315, 0.1543566326398047, 0.15814437161835648, 0.16673068928307394, 0.1730018521396129, 0.17574932026987625, 0.17918384180196628, 0.18063656622301857, 0.18320902931477656, 0.18475045583918479, 0.19185534065356152, 0.19729392254070355, 0.20319703852958604, 0.20842070936066248, 0.21417759515132923, 0.21918813172798046, 0.22015682501385292, 0.22397475477660178, 0.22852889167214643, 0.23447292342091236, 0.23666853844567662, 0.24413277888623264, 0.24729704524071738, 0.24853790333471396, 0.25123877825582164, 0.25346940294300924, 0.26367119675301054, 0.2684739334167857, 0.27557630381138265, 0.28038343908158125, 0.2879312289549069, 0.29157280125984336, 0.29344074358881217, 0.30011545049990745, 0.30011545049990745, 0.30491617069317545, 0.306538999854953, 0.31163852514675583, 0.314067742282543, 0.3229843338198085, 0.33052268576522703, 0.33052268576522703, 0.33280855529874287, 0.33280855529874287, 0.3387733881955836, 0.34134731941102087, 0.34134731941102087, 0.34646139712825696, 0.34646139712825696, 0.34646139712825696, 0.34646139712825696, 0.34646139712825696, 0.34646139712825696, 0.34646139712825696, 0.3537617774235766, 0.3537617774235766, 0.35772121908361976, 0.3616314387985918, 0.3616314387985918, 0.3616314387985918, 0.3616314387985918, 0.3616314387985918, 0.3616314387985918, 0.3665819557394877, 0.3665819557394877, 0.3665819557394877, 0.38083575542388937, 0.38083575542388937, 0.38083575542388937, 0.3860783862803089, 0.3860783862803089, 0.3860783862803089, 0.3860783862803089, 0.3860783862803089, 0.3922022117539085, 0.3922022117539085, 0.3922022117539085, 0.3922022117539085, 0.3922022117539085, 0.3922022117539085, 0.3922022117539085, 0.3922022117539085, 0.3922022117539085, 0.3922022117539085, 0.3922022117539085, 0.3922022117539085]
Training raw path: /home/anhnd/DTI Project/Codes/MPNN/data/KFold/ATCInchikeySideEffectByDrug.txt_train_1
('Number of substructures, proteins, pathways, drugs, se: ', 888, 1448, 330, 969, 598)
((872, 598), (872, 598), 49787.13, 89035.0)
('Error: ', tensor(77240.0938, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.4863210489989996, 0.1629207992215609)
('Test: AUC, AUPR: ', 0.49951983292454605, 0.16996486642259787)
((872, 598), (872, 598), 23119.654, 89035.0)
('Error: ', tensor(80344.3984, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.7122579462324372, 0.3963131685918262)
('Test: AUC, AUPR: ', 0.6997802449648151, 0.3830937619708708)
((872, 598), (872, 598), 131494.58, 89035.0)
('Error: ', tensor(67490.2188, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.7511047122804293, 0.45210943892673366)
('Test: AUC, AUPR: ', 0.7426636588314595, 0.44037556600494876)
((872, 598), (872, 598), 87984.84, 89035.0)
('Error: ', tensor(60498.3672, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.7748379279798325, 0.4919417127376557)
('Test: AUC, AUPR: ', 0.759507939102035, 0.4646087811825388)
((872, 598), (872, 598), 75337.78, 89035.0)
('Error: ', tensor(59047.8125, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.7828788892756827, 0.5128227122429463)
('Test: AUC, AUPR: ', 0.7628471591820167, 0.47204213991577826)
((872, 598), (872, 598), 85285.74, 89035.0)
('Error: ', tensor(56911.7109, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.800712089573678, 0.5442081807586973)
('Test: AUC, AUPR: ', 0.7650284218701972, 0.46970382460468874)
((872, 598), (872, 598), 91482.82, 89035.0)
('Error: ', tensor(55399.2383, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8195218151557644, 0.5730670008013777)
('Test: AUC, AUPR: ', 0.759515453160672, 0.4498801607270705)
((872, 598), (872, 598), 96419.25, 89035.0)
('Error: ', tensor(54385.7188, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8273231748582605, 0.5856049048635658)
('Test: AUC, AUPR: ', 0.7488140310784595, 0.42107001533941396)
((872, 598), (872, 598), 96936.56, 89035.0)
('Error: ', tensor(53520.0742, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8340240969600472, 0.5971107476748125)
('Test: AUC, AUPR: ', 0.7367529163150295, 0.3861622011170313)
