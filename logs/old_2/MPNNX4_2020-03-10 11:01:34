    def __init__(self, numNode=10000, numAtomFeature=0):
        super(Net4, self).__init__()

        self.convD1 = GATConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.convD2 = GATConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)

        self.convS1 = GATConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.convS2 = GATConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)

        self.L1 = Linear(config.CHEM_FINGERPRINT_SIZE, config.EMBED_DIM * 2)
        self.actL1 = F.relu
        self.L2 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.actL2 = F.relu

        self.linear1 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.act1 = F.relu
        self.linear2 = Linear(config.EMBED_DIM, 1)
        self.act2 = F.relu

        self.nodesEmbedding = torch.nn.Embedding(num_embeddings=numNode + 1, embedding_dim=config.EMBED_DIM)
        self.nodesEmbedding.weight.data.uniform_(0.001, 0.3)



        # Molecule graph neural net

        self.mlinear1 = Linear(numAtomFeature, config.EMBED_DIM * 2)
        self.mact1 = F.relu
        self.mlinear2 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.mact2 = F.relu

        self.conv1 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool1 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.conv2 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool2 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.conv3 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool3 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.lin1 = torch.nn.Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.lin2 = torch.nn.Linear(config.EMBED_DIM, config.EMBED_DIM)


        self.bn1 = torch.nn.BatchNorm1d(128)
        self.bn2 = torch.nn.BatchNorm1d(64)
        self.act1 = torch.nn.ReLU()
        self.act2 = torch.nn.ReLU()

    def forward(self, x, drugEdges, seEdges, drugNodes, seNodes, drugGraphBatch, nDrug):
        # x = self.nodesEmbedding(x[nDrug:])
        # x = x.squeeze(1)
        #
        #
        # xDrug, edge_index, batch = drugGraphBatch.x, drugGraphBatch.edge_index, drugGraphBatch.batch
        #
        # xDrug = self.mact1(self.mlinear1(xDrug))
        # xDrug = self.mact2(self.mlinear2(xDrug))
        #
        # xDrug = F.relu(self.conv1(xDrug, edge_index))
        #
        # v  = self.pool1(xDrug, edge_index, None, batch)
        # xDrug, edge_index, _, batch, _, _ = v
        # x1 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)
        #
        # xDrug = F.relu(self.conv2(xDrug, edge_index))
        #
        # xDrug, edge_index, _, batch, _, _ = self.pool2(xDrug, edge_index, None, batch)
        # x2 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)
        #
        # xDrug = F.relu(self.conv3(xDrug, edge_index))
        #
        # xDrug, edge_index, _, batch, _, _ = self.pool3(xDrug, edge_index, None, batch)
        # x3 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)
        #
        # xDrug = x1 + x2 + x3
        #
        # xDrug = self.lin1(xDrug)
        # xDrug = self.act1(xDrug)
        # xDrug = self.lin2(xDrug)
        # xDrug = self.act2(xDrug)
        #
        #
        #
        # x = torch.cat((xDrug, x), dim=0)
        #
        # # Conv Drug:
        # x = self.convD1(x, drugEdges)
        # x = F.relu(x)
        # x = self.convD2(x, drugEdges)
        # x = F.relu(x)
        # # Conv SE:
        # x = self.convS1(x, seEdges)
        # x = F.relu(x)
        # x = self.convS2(x, seEdges)
        # x = F.relu(x)

        x = self.nodesEmbedding(x)
        drugEmbedding = x[drugNodes]
        seEmbedding = x[seNodes]
        # re = torch.sigmoid(re)
        return drugEmbedding, seEmbedding, x

('Undirected graph: ', False)
MPNNX
<models.MPNNX4.MPNNX4 object at 0x7fd762ec59d0>
Training raw path: /home/anhnd/DTI Project/Codes/MPNN/data/KFold/ATCInchikeySideEffectByDrug.txt_train_0
('Number of substructures, proteins, pathways, drugs, se: ', 2936, 1448, 330, 969, 598)
((872, 598), (872, 598), 591254.2, 88676.0)
('Error: ', tensor(568242.8750, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.5034516822093642, 0.17255329745668718)
('Test: AUC, AUPR: ', 0.4963768855029233, 0.180285690488677)
((872, 598), (872, 598), 119015.336, 88676.0)
('Error: ', tensor(71915.8359, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.6414647434897442, 0.3164568181157722)
('Test: AUC, AUPR: ', 0.6085099688247949, 0.30944048039524497)
((872, 598), (872, 598), 33658.277, 88676.0)
('Error: ', tensor(69816.5312, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.7422455965629808, 0.4891989886891586)
('Test: AUC, AUPR: ', 0.7036049933066577, 0.42951416698917677)
((872, 598), (872, 598), 30641.293, 88676.0)
('Error: ', tensor(66027.1797, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.7826474654093102, 0.568596979580476)
('Test: AUC, AUPR: ', 0.7288201156485568, 0.44881827963048854)
((872, 598), (872, 598), 52956.305, 88676.0)
('Error: ', tensor(57536.0508, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8218130238971415, 0.6110897450695765)
('Test: AUC, AUPR: ', 0.7371687173936665, 0.4517829551209307)
((872, 598), (872, 598), 83308.33, 88676.0)
('Error: ', tensor(52508.4336, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8397731530475561, 0.6205018120645536)
('Test: AUC, AUPR: ', 0.7395132242640676, 0.44898258168992156)
((872, 598), (872, 598), 98166.375, 88676.0)
('Error: ', tensor(51457.0859, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8452566396215665, 0.6222560943702837)
('Test: AUC, AUPR: ', 0.7393411632826468, 0.4413173599380419)
((872, 598), (872, 598), 97670.31, 88676.0)
('Error: ', tensor(50734.6484, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8532139919356825, 0.6359212699337053)
('Test: AUC, AUPR: ', 0.7390459754236656, 0.44453195622363184)
((872, 598), (872, 598), 95169.96, 88676.0)
('Error: ', tensor(50023.1250, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8614873987359923, 0.6506544753735408)
('Test: AUC, AUPR: ', 0.7388833277355282, 0.4482844661204506)
((872, 598), (872, 598), 94055.3, 88676.0)
('Error: ', tensor(49288.7656, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8675688645380482, 0.6604384741990993)
('Test: AUC, AUPR: ', 0.7386453977350642, 0.4490320298280336)
((872, 598), (872, 598), 93507.66, 88676.0)
('Error: ', tensor(48323.1797, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8732478562724341, 0.6705176523225269)
('Test: AUC, AUPR: ', 0.738395750187291, 0.4482721029154305)
((872, 598), (872, 598), 93322.87, 88676.0)
('Error: ', tensor(47097.8711, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.880339481068041, 0.6851565329756749)
('Test: AUC, AUPR: ', 0.7381888940688038, 0.44783430274135144)
((872, 598), (872, 598), 92740.13, 88676.0)
('Error: ', tensor(45618.7656, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8888807911336463, 0.7035340292560869)
('Test: AUC, AUPR: ', 0.7379344297188292, 0.4479583438516376)
((872, 598), (872, 598), 91506.336, 88676.0)
('Error: ', tensor(44026.6953, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8978201735908438, 0.7226175744771163)
('Test: AUC, AUPR: ', 0.7375504264363056, 0.4480213556693387)
((872, 598), (872, 598), 90221.28, 88676.0)
('Error: ', tensor(42470.7734, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.9063202134353354, 0.7405348833471967)
('Test: AUC, AUPR: ', 0.7370917306744073, 0.447728778833974)
((872, 598), (872, 598), 89188.76, 88676.0)
('Error: ', tensor(41000.2969, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.9142322494410018, 0.7574396705367639)
('Test: AUC, AUPR: ', 0.7366385266678669, 0.44722467765959917)
((872, 598), (872, 598), 88540.88, 88676.0)
('Error: ', tensor(39637.2500, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.9215039326861477, 0.7733911444311448)
('Test: AUC, AUPR: ', 0.7362003048188597, 0.446563752800732)
((872, 598), (872, 598), 88407.266, 88676.0)
('Error: ', tensor(38395.7383, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.9279636123175687, 0.7880527348378566)
('Test: AUC, AUPR: ', 0.7357719368853748, 0.445851324289021)
((872, 598), (872, 598), 88540.55, 88676.0)
('Error: ', tensor(37260.8086, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.9336165318393187, 0.8014725291964522)
('Test: AUC, AUPR: ', 0.7353262234568938, 0.4451836388972304)
Train: 0.9336 0.8015
Test: 0.7353 0.4452
[0.48172785462313183, 0.5000556807949504, 0.5046056779810953, 0.510220813003224, 0.5144099057046901, 0.527125539606943, 0.5379397269650157, 0.5492780784282082, 0.5538960510491587, 0.5595376529011707, 0.5637850895100445, 0.569222047541366, 0.5819302814836871, 0.5898692066670358, 0.592852617615469, 0.5966920931573371, 0.59871770982762, 0.6026696320768207, 0.6054452815465527, 0.6128101148343057, 0.6190078335595917, 0.6241227998842803, 0.6293665801810113, 0.6337802172825492, 0.637991708796434, 0.6386217327336049, 0.6419314173926702, 0.6454890056414373, 0.650166049808597, 0.651701672197014, 0.657760922275119, 0.6606177472225256, 0.6616832783262616, 0.663544505137249, 0.6648049955173518, 0.6717677550301087, 0.6746402055571998, 0.6790969433095683, 0.6819218514479239, 0.6864091151727257, 0.6882975864855049, 0.6895907353231306, 0.693774443217249, 0.693774443217249, 0.6964173166245546, 0.6975566447704094, 0.6999422587214983, 0.701201561258272, 0.7056355433715794, 0.7093310567246046, 0.7093310567246046, 0.7106891761316108, 0.7106891761316108, 0.713420961639906, 0.7146313176920602, 0.7146313176920602, 0.7169814580591742, 0.7169814580591742, 0.7169814580591742, 0.7169814580591742, 0.7169814580591742, 0.7169814580591742, 0.7169814580591742, 0.7202715485004381, 0.7202715485004381, 0.721954282369153, 0.7235606239652312, 0.7235606239652312, 0.7235606239652312, 0.7235606239652312, 0.7235606239652312, 0.7235606239652312, 0.7254777239887413, 0.7254777239887413, 0.7254777239887413, 0.7310876772722317, 0.7310876772722317, 0.7310876772722317, 0.7331217833488649, 0.7331217833488649, 0.7331217833488649, 0.7331217833488649, 0.7331217833488649, 0.7353262234568938, 0.7353262234568938, 0.7353262234568938, 0.7353262234568938, 0.7353262234568938, 0.7353262234568938, 0.7353262234568938, 0.7353262234568938, 0.7353262234568938, 0.7353262234568938, 0.7353262234568938, 0.7353262234568938]
[0.05559995388462015, 0.06572918636465097, 0.06968571939908055, 0.07578830711500276, 0.07965285289344697, 0.08894791477198676, 0.09698687255061597, 0.10576847784137434, 0.1088407196892402, 0.11414172602913042, 0.1175924018993548, 0.12262374866993779, 0.13470583184273588, 0.14126324021120482, 0.14429189028885728, 0.14860551363005123, 0.15075547788576443, 0.15599355841260262, 0.15933141526207711, 0.16826506399052843, 0.1771195262854351, 0.18397980925135665, 0.19115253462027604, 0.19709170733983905, 0.2028596015405828, 0.2036810807536192, 0.20862758966312542, 0.21482909307660936, 0.22223428852269989, 0.22455305946267942, 0.23617337979854572, 0.24059747845976504, 0.2422362401806068, 0.24673446981248923, 0.24986926016854516, 0.2654098983366996, 0.2706781059500609, 0.2795514177872428, 0.28510348388237733, 0.29514577350101434, 0.2988515949942282, 0.3018476182257869, 0.3107391920553916, 0.3107391920553916, 0.317893847156721, 0.32036449690243074, 0.32647296366169914, 0.3304979550962885, 0.342919275049402, 0.3523274622074668, 0.3523274622074668, 0.3561498409606375, 0.3561498409606375, 0.36457581321794685, 0.36768224415144923, 0.36768224415144923, 0.3742948413266119, 0.3742948413266119, 0.3742948413266119, 0.3742948413266119, 0.3742948413266119, 0.3742948413266119, 0.3742948413266119, 0.38567340658772065, 0.38567340658772065, 0.3921453353066338, 0.39790287420910925, 0.39790287420910925, 0.39790287420910925, 0.39790287420910925, 0.39790287420910925, 0.39790287420910925, 0.40608022879211597, 0.40608022879211597, 0.40608022879211597, 0.4277192838594547, 0.4277192838594547, 0.4277192838594547, 0.43566144895352443, 0.43566144895352443, 0.43566144895352443, 0.43566144895352443, 0.43566144895352443, 0.4451836388972304, 0.4451836388972304, 0.4451836388972304, 0.4451836388972304, 0.4451836388972304, 0.4451836388972304, 0.4451836388972304, 0.4451836388972304, 0.4451836388972304, 0.4451836388972304, 0.4451836388972304, 0.4451836388972304]
