    def __init__(self, numNode=10000, numAtomFeature=0):
        super(Net4, self).__init__()

        self.convD1 = GATConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.convD2 = GATConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)

        self.convS1 = GATConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.convS2 = GATConv(config.EMBED_DIM, config.EMBED_DIM)  # SAGEConv(config.EMBED_DIM, config.EMBED_DIM)

        self.L1 = Linear(config.CHEM_FINGERPRINT_SIZE, config.EMBED_DIM * 2)
        self.actL1 = F.relu
        self.L2 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.actL2 = F.relu

        self.linear1 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.act1 = F.relu
        self.linear2 = Linear(config.EMBED_DIM, 1)
        self.act2 = F.relu

        self.nodesEmbedding = torch.nn.Embedding(num_embeddings=numNode + 1, embedding_dim=config.EMBED_DIM)
        self.nodesEmbedding.weight.data.uniform_(0.001, 0.3)



        # Molecule graph neural net

        self.mlinear1 = Linear(numAtomFeature, config.EMBED_DIM * 2)
        self.mact1 = F.relu
        self.mlinear2 = Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.mact2 = F.relu

        self.conv1 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool1 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.conv2 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool2 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.conv3 = SAGEConv(config.EMBED_DIM, config.EMBED_DIM)
        self.pool3 = TopKPooling(config.EMBED_DIM, ratio=0.8)
        self.lin1 = torch.nn.Linear(config.EMBED_DIM * 2, config.EMBED_DIM)
        self.lin2 = torch.nn.Linear(config.EMBED_DIM, config.EMBED_DIM)


        self.bn1 = torch.nn.BatchNorm1d(128)
        self.bn2 = torch.nn.BatchNorm1d(64)
        self.act1 = torch.nn.ReLU()
        self.act2 = torch.nn.ReLU()

    def forward(self, x, drugEdges, seEdges, drugNodes, seNodes, drugGraphBatch, nDrug):
        # x = self.nodesEmbedding(x[nDrug:])
        # x = x.squeeze(1)
        #
        #
        # xDrug, edge_index, batch = drugGraphBatch.x, drugGraphBatch.edge_index, drugGraphBatch.batch
        #
        # xDrug = self.mact1(self.mlinear1(xDrug))
        # xDrug = self.mact2(self.mlinear2(xDrug))
        #
        # xDrug = F.relu(self.conv1(xDrug, edge_index))
        #
        # v  = self.pool1(xDrug, edge_index, None, batch)
        # xDrug, edge_index, _, batch, _, _ = v
        # x1 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)
        #
        # xDrug = F.relu(self.conv2(xDrug, edge_index))
        #
        # xDrug, edge_index, _, batch, _, _ = self.pool2(xDrug, edge_index, None, batch)
        # x2 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)
        #
        # xDrug = F.relu(self.conv3(xDrug, edge_index))
        #
        # xDrug, edge_index, _, batch, _, _ = self.pool3(xDrug, edge_index, None, batch)
        # x3 = torch.cat([gmp(xDrug, batch), gap(xDrug, batch)], dim=1)
        #
        # xDrug = x1 + x2 + x3
        #
        # xDrug = self.lin1(xDrug)
        # xDrug = self.act1(xDrug)
        # xDrug = self.lin2(xDrug)
        # xDrug = self.act2(xDrug)
        #
        #
        #
        # x = torch.cat((xDrug, x), dim=0)
        #
        # # Conv Drug:
        # x = self.convD1(x, drugEdges)
        # x = F.relu(x)
        # x = self.convD2(x, drugEdges)
        # x = F.relu(x)
        # # Conv SE:
        # x = self.convS1(x, seEdges)
        # x = F.relu(x)
        # x = self.convS2(x, seEdges)
        # x = F.relu(x)

        x = self.nodesEmbedding(x)
        drugEmbedding = x[drugNodes]
        seEmbedding = x[seNodes]
        # re = torch.sigmoid(re)
        return drugEmbedding, seEmbedding, x

('Undirected graph: ', False)
MPNNX
<models.MPNNX4.MPNNX4 object at 0x7fb7f11b1910>
Training raw path: /home/anhnd/DTI Project/Codes/MPNN/data/KFold/ATCInchikeySideEffectByDrug.txt_train_0
('Number of substructures, proteins, pathways, drugs, se: ', 2936, 1448, 330, 969, 598)
((872, 598), (872, 598), 588743.2, 88676.0)
('Error: ', tensor(564730.3750, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.48202936647439487, 0.16066917987629095)
('Test: AUC, AUPR: ', 0.5303211409694966, 0.19057331628964047)
((872, 598), (872, 598), 118284.086, 88676.0)
('Error: ', tensor(72289.1953, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.63244716478435, 0.30302592978434867)
('Test: AUC, AUPR: ', 0.628560504830833, 0.3193434389072527)
((872, 598), (872, 598), 33788.21, 88676.0)
('Error: ', tensor(69781.2109, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.7418501615837558, 0.48883771137867604)
('Test: AUC, AUPR: ', 0.7124912152693615, 0.4399923027421184)
((872, 598), (872, 598), 31202.555, 88676.0)
('Error: ', tensor(65833.5547, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.7834959109866548, 0.5686796871604645)
('Test: AUC, AUPR: ', 0.7342500717348026, 0.45700159203126645)
((872, 598), (872, 598), 53842.734, 88676.0)
('Error: ', tensor(57456.1289, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8230476680449413, 0.6118132947878864)
('Test: AUC, AUPR: ', 0.7420661614760455, 0.46008167720465865)
((872, 598), (872, 598), 83769.39, 88676.0)
('Error: ', tensor(52401.4453, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8403065415929434, 0.6209127921968717)
('Test: AUC, AUPR: ', 0.7445190403874926, 0.4578384670182119)
((872, 598), (872, 598), 98231.125, 88676.0)
('Error: ', tensor(51415.9570, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.845303413423034, 0.62240733253908)
('Test: AUC, AUPR: ', 0.7449993456659613, 0.4513580131204482)
((872, 598), (872, 598), 97827.94, 88676.0)
('Error: ', tensor(50678.8828, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.853344147916153, 0.6366169416670296)
('Test: AUC, AUPR: ', 0.7454876542465492, 0.45675488954773713)
((872, 598), (872, 598), 95188.32, 88676.0)
('Error: ', tensor(49929.6484, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8616699083805576, 0.6514284953332297)
('Test: AUC, AUPR: ', 0.7458629261988453, 0.4604846002682843)
((872, 598), (872, 598), 93766.29, 88676.0)
('Error: ', tensor(49150.0859, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8677874865364589, 0.6615081650413213)
('Test: AUC, AUPR: ', 0.7459531536163803, 0.46078808091709356)
((872, 598), (872, 598), 93234.516, 88676.0)
('Error: ', tensor(48117.4609, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8736013834644788, 0.6722430311189301)
('Test: AUC, AUPR: ', 0.7459372131054778, 0.46011879288445245)
((872, 598), (872, 598), 93156.59, 88676.0)
('Error: ', tensor(46826.6602, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8808625030023296, 0.6873979887182474)
('Test: AUC, AUPR: ', 0.745964862150962, 0.46015038349185344)
((872, 598), (872, 598), 92414.04, 88676.0)
('Error: ', tensor(45317.4336, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8895118825356867, 0.7057350330345159)
('Test: AUC, AUPR: ', 0.7459961483327449, 0.4607313260859715)
((872, 598), (872, 598), 90974.23, 88676.0)
('Error: ', tensor(43751.8242, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.8984640138518206, 0.7243000876773704)
('Test: AUC, AUPR: ', 0.745948528084287, 0.4609867188035097)
((872, 598), (872, 598), 89659.29, 88676.0)
('Error: ', tensor(42246.5508, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.9069329681814133, 0.7418596442678256)
('Test: AUC, AUPR: ', 0.7458148743408765, 0.4608577595650075)
((872, 598), (872, 598), 88727.15, 88676.0)
('Error: ', tensor(40818.9336, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.9148099263798075, 0.7588480851313613)
('Test: AUC, AUPR: ', 0.7455980130933262, 0.4605349152999862)
((872, 598), (872, 598), 88246.055, 88676.0)
('Error: ', tensor(39484.8320, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.922025230823462, 0.7750541186867098)
('Test: AUC, AUPR: ', 0.7453256775059116, 0.46004985166998685)
((872, 598), (872, 598), 88243.72, 88676.0)
('Error: ', tensor(38257.6406, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.9284306339954468, 0.7899230680282883)
('Test: AUC, AUPR: ', 0.7450129208365728, 0.45944069729523146)
((872, 598), (872, 598), 88397.94, 88676.0)
('Error: ', tensor(37131.3906, grad_fn=<AddBackward0>))
('Train: AUC, AUPR: ', 0.9340491333399876, 0.8033853677846364)
('Test: AUC, AUPR: ', 0.74468793990435, 0.45882595026647294)
Train: 0.9340 0.8034
Test: 0.7447 0.4588
[0.49816187918857324, 0.5052220314776187, 0.517753525328535, 0.5329424266811912, 0.5425031999342398, 0.5532123836780415, 0.564294617699831, 0.5734590380240682, 0.5778567171218049, 0.582729251895392, 0.5861061884749957, 0.5901835144007915, 0.601014145718092, 0.6080033310444859, 0.6105839111352315, 0.6140724110266202, 0.6156645404407236, 0.6190849065408537, 0.6215383175694931, 0.6284321446385437, 0.6341617146687097, 0.6387222296970179, 0.643752566835469, 0.6478469124406758, 0.6519799244404847, 0.6525606273788891, 0.6557061143336784, 0.6591033469512895, 0.6636337154044747, 0.6651282677806464, 0.6708366692499482, 0.6736247354396879, 0.6746895524558716, 0.6764241290369368, 0.6776143335852177, 0.6842003699725534, 0.686876437621419, 0.691089097196357, 0.6937179639738474, 0.6979706277424171, 0.6997895090589155, 0.7010286611033766, 0.7050414300287764, 0.7050414300287764, 0.707511527883213, 0.7085879371938681, 0.710872267224997, 0.7120626887545601, 0.7162275795047566, 0.7197642962706357, 0.7197642962706357, 0.7210788193859536, 0.7210788193859536, 0.7236804930397231, 0.7248450860733161, 0.7248450860733161, 0.7270793264895302, 0.7270793264895302, 0.7270793264895302, 0.7270793264895302, 0.7270793264895302, 0.7270793264895302, 0.7270793264895302, 0.7302404110274789, 0.7302404110274789, 0.7318489140846366, 0.7333897095675646, 0.7333897095675646, 0.7333897095675646, 0.7333897095675646, 0.7333897095675646, 0.7333897095675646, 0.7352318025358011, 0.7352318025358011, 0.7352318025358011, 0.7406076352298061, 0.7406076352298061, 0.7406076352298061, 0.7425653096254682, 0.7425653096254682, 0.7425653096254682, 0.7425653096254682, 0.7425653096254682, 0.74468793990435, 0.74468793990435, 0.74468793990435, 0.74468793990435, 0.74468793990435, 0.74468793990435, 0.74468793990435, 0.74468793990435, 0.74468793990435, 0.74468793990435, 0.74468793990435, 0.74468793990435]
[0.05920730311830335, 0.06586253483708746, 0.07481778122568172, 0.08454723324424175, 0.08957821204022387, 0.09827147623711702, 0.10625068855584874, 0.11365858509838575, 0.11731774736696315, 0.12270266584389489, 0.12733539604804042, 0.1310470619729905, 0.14483329283179125, 0.15300996656844715, 0.15583627526036511, 0.16165444663389544, 0.16354509092931202, 0.1677226745591115, 0.17105763665780577, 0.1832524916252089, 0.19213230245657573, 0.19919807510676713, 0.20693215818871227, 0.21301283678677413, 0.21922445500437615, 0.22000337032914732, 0.22430965731684036, 0.23170046828241003, 0.24093137819970878, 0.2434792088321871, 0.25454346376532927, 0.25982262511225057, 0.262104150620143, 0.26569815878395536, 0.26892246310154305, 0.28374787741216456, 0.2878672306232557, 0.29766256971219146, 0.3029037933463241, 0.3128786401609402, 0.3173522953451957, 0.32038543317717044, 0.3306278117614335, 0.3306278117614335, 0.3355571990332612, 0.3382539117249486, 0.34496630789890836, 0.3485070797959815, 0.3575965339436584, 0.3672849657100904, 0.3672849657100904, 0.3715545540142384, 0.3715545540142384, 0.38008553809894396, 0.3833589139477514, 0.3833589139477514, 0.38955920797587007, 0.38955920797587007, 0.38955920797587007, 0.38955920797587007, 0.38955920797587007, 0.38955920797587007, 0.38955920797587007, 0.4014889856786492, 0.4014889856786492, 0.4075706404476426, 0.4126952301228343, 0.4126952301228343, 0.4126952301228343, 0.4126952301228343, 0.4126952301228343, 0.4126952301228343, 0.42096616344350324, 0.42096616344350324, 0.42096616344350324, 0.4415901481743848, 0.4415901481743848, 0.4415901481743848, 0.4497079195897618, 0.4497079195897618, 0.4497079195897618, 0.4497079195897618, 0.4497079195897618, 0.45882595026647294, 0.45882595026647294, 0.45882595026647294, 0.45882595026647294, 0.45882595026647294, 0.45882595026647294, 0.45882595026647294, 0.45882595026647294, 0.45882595026647294, 0.45882595026647294, 0.45882595026647294, 0.45882595026647294]
